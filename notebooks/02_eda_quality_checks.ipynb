"""
# 02 — EDA & Quality Checks (sin split)

Objetivo:
- Revisar calidad de datos (missing / tipos / outliers)
- Analizar target (`cultivo_grupo`) y sesgos geográficos (provincia)
- PSI por provincia vs global
- Correlaciones numéricas
- Guardar artefactos en `outputs/eda/analysis/`
"""

import pandas as pd
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns

OUTPUT_DIR = Path("outputs/eda")
ANALYSIS_DIR = OUTPUT_DIR / "analysis"
ANALYSIS_DIR.mkdir(parents=True, exist_ok=True)

DATA_FULL = OUTPUT_DIR / "dataset_final_2017_full.csv"
DATA_MODEL = OUTPUT_DIR / "model" / "dataset_final_2017_model.csv"

print(DATA_FULL.exists(), DATA_MODEL.exists())

df_full = pd.read_csv(DATA_FULL, low_memory=False)
df_model = pd.read_csv(DATA_MODEL, low_memory=False)

print("FULL:", df_full.shape)
print("MODEL:", df_model.shape)

df_full.head(3)

## 1) Tipos + resumen rápido
display(df_model.dtypes.value_counts())
display(df_model.describe(include="all").T.head(20))

## 2) Missingness
miss = (df_model.isna().mean().sort_values(ascending=False)*100).round(2).to_frame("missing_pct")
miss.to_csv(ANALYSIS_DIR / "missing_rates.csv", index=True)
display(miss.head(30))

plt.figure(figsize=(12,4))
miss.head(40).plot(kind="bar", legend=False)
plt.title("Missing % (top 40 variables)"); plt.ylabel("%"); plt.tight_layout()
plt.savefig(ANALYSIS_DIR / "missing_top40.png"); plt.close()

## 3) Target check: balance de clases
target = "cultivo_grupo"
vc = df_model[target].fillna("NA").value_counts()
dist = vc.to_frame("n").assign(pct=(100*vc/vc.sum()).round(2))
dist.to_csv(ANALYSIS_DIR / "target_balance.csv", index=True)
display(dist)

plt.figure(figsize=(10,4))
sns.barplot(x=vc.index, y=vc.values)
plt.xticks(rotation=45, ha="right")
plt.title("Balance de clases (cultivo_grupo)")
plt.tight_layout()
plt.savefig(ANALYSIS_DIR / "target_balance.png"); plt.close()

## 4) Outliers rápidos (IQR) en numéricas “fiables”
def to_num(s):
    return pd.to_numeric(s.astype(str).str.replace(",", ".", regex=False).str.replace("%","", regex=False), errors="coerce")

num_cols = []
for c in df_model.columns:
    s = to_num(df_model[c])
    if s.notna().mean() >= 0.8:
        num_cols.append(c)

rows = []
for c in num_cols:
    s = to_num(df_model[c]).dropna()
    if s.empty: 
        continue
    q1, q3 = s.quantile([0.25, 0.75])
    iqr = q3 - q1
    if iqr == 0 or not np.isfinite(iqr):
        out = 0.0
    else:
        lb, ub = q1 - 1.5*iqr, q3 + 1.5*iqr
        out = ((s < lb) | (s > ub)).mean()
    rows.append({"variable": c, "outlier_pct": round(100*out, 2)})

outliers = pd.DataFrame(rows).sort_values("outlier_pct", ascending=False)
outliers.to_csv(ANALYSIS_DIR / "outlier_rates_iqr.csv", index=False)
display(outliers.head(30))

## 5) PSI por provincia (global vs provincia)
def psi_from_counts(base_counts, comp_counts):
    base_ratio = base_counts / (base_counts.sum() + 1e-12)
    comp_ratio = comp_counts / (comp_counts.sum() + 1e-12)
    base_ratio = np.maximum(base_ratio, 1e-6)
    comp_ratio = np.maximum(comp_ratio, 1e-6)
    return float(np.sum((comp_ratio - base_ratio) * np.log(comp_ratio / base_ratio)))

def psi_numeric(base, comp, bins=10):
    b = to_num(base).dropna()
    if b.empty: 
        return np.nan
    edges = np.unique(b.quantile(np.linspace(0,1,bins+1)).values)
    if len(edges) < 3:
        edges = np.array([-np.inf, b.median(), np.inf])
    edges[0], edges[-1] = -np.inf, np.inf
    bc = pd.cut(to_num(base), bins=edges).value_counts(sort=False).values
    cc = pd.cut(to_num(comp), bins=edges).value_counts(sort=False).values
    return psi_from_counts(bc.astype(float), cc.astype(float))

def psi_categorical(base, comp):
    bvc = base.astype(str).value_counts()
    cvc = comp.astype(str).value_counts()
    cats = sorted(set(bvc.index).union(cvc.index))
    bc = np.array([bvc.get(k,0) for k in cats], dtype=float)
    cc = np.array([cvc.get(k,0) for k in cats], dtype=float)
    return psi_from_counts(bc, cc)

if "station_provincia" in df_model.columns:
    clima_cols = [c for c in df_model.columns if c.endswith("_2017") or c=="n_dias_lluvia"]
    psi_rows = []
    for c in clima_cols:
        for prov, dfg in df_model.groupby(df_model["station_provincia"].astype(str)):
            psi_rows.append({"variable": c, "group": prov, "psi": psi_numeric(df_model[c], dfg[c])})
    # target psi
    for prov, dfg in df_model.groupby(df_model["station_provincia"].astype(str)):
        psi_rows.append({"variable": f"PSI_{target}", "group": prov, "psi": psi_categorical(df_model[target], dfg[target])})

    psi_df = pd.DataFrame(psi_rows).sort_values(["variable","psi"], ascending=[True, False])
    psi_df.to_csv(ANALYSIS_DIR / "psi_by_provincia.csv", index=False)
    display(psi_df.head(30))
else:
    print("No existe station_provincia en el dataset.")

## 6) Correlación (numéricas)
num_for_corr = []
tmp = df_model.copy()

for c in df_model.columns:
    s = to_num(df_model[c])
    if s.notna().mean() >= 0.8:
        tmp[c] = s
        num_for_corr.append(c)

corr = tmp[num_for_corr].corr(numeric_only=True)
corr.to_csv(ANALYSIS_DIR / "corr_matrix.csv", index=True)

plt.figure(figsize=(min(18, 0.4*len(num_for_corr)+6), min(18, 0.4*len(num_for_corr)+6)))
sns.heatmap(corr, center=0)
plt.title("Matriz de correlación (numéricas)")
plt.tight_layout()
plt.savefig(ANALYSIS_DIR / "corr_matrix.png"); plt.close()

display(corr.iloc[:10, :10])

